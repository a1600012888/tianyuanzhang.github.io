<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tianyuan Zhang</title>

  <meta name="author" content="Tianyuan Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/pku.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Tianyuan Zhang 「张天远」</name>
                  </p>
                  <p>I am a second-year master student in Robotics Institute at CMU, supervised by <a
                      href="http://www.cs.cmu.edu/~srinivas/"> Prof.
                      Srinivasa Narasimhan</a>.
                    Previously, I finished my undergraduate in School of EECS at Peking
                    University, working with <a href="https://scholar.google.co.uk/citations?user=a2sHceIAAAAJ&hl=en">
                      Prof. Zhanxing Zhu</a>
                    <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=zh-CN" Dr. Xiangyu Zhang></a>,
                    and
                    <a href="https://hangzhaomit.github.io/"> Prof. Hang Zhao</a>.
                  </p>
                  <p>
                    I work on computational imaging, and computer vision.
                  </p>
                  <p>Email: tianyuaz [at] andrew [dot] cmu [dot] edu
                  </p>
                  <p style="text-align:center">
                    <a href="data/cv-july-2022.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=uJocZjkAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/a1600012888">Github</a>&nbsp/&nbsp
                    <a href="portfolio.html" target="_blank">
                      Portfolio</a> &nbsp&nbsp&nbsp&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/selffile_baseball.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/selffile_baseball.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested in physiscs-based vision, computational imaging and computer graphics.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr onmouseout="rife_stop()" onmouseover="rife_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='rife_gif'>
                      <img src='images/rife/rife_demo.gif' height="128" width="184">
                    </div>
                    <img src='images/rife/rife_demo.gif' height="128" width="184">
                  </div>
                  <script type="text/javascript">
                    function rife_start() {
                      document.getElementById('rife_gif').style.opacity = "1";
                    }

                    function rife_stop() {
                      document.getElementById('rife_gif').style.opacity = "0";
                    }
                    dreamfusion_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/megvii-research/ECCV2022-RIFE/">
                    <papertitle>Real-Time Intermediate Flow Estimation for Video Frame Interpolation</papertitle>
                  </a>
                  <br>
                  Zhewei Huang,
                  <strong>Tianyuan Zhang</strong>,
                  Wen Heng, Boxin Shi, Shuchang Zhou
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="https://github.com/megvii-research/ECCV2022-RIFE/">github</a>
                  /
                  <a href="https://arxiv.org/abs/2011.06294">arXiv</a>
                  /
                  <a href="https://www.youtube.com/results?search_query=rife+interpolation&sp=CAM%253D">demos</a>
                  <p></p>
                  <p>
                    We propose a real-time intermediate flow estimation (RIFE) method for video frame interpolation, it
                    runs 30+FPS
                    for 2X 720p interpolation on a 2080Ti GPU
                  </p>
                </td>
              </tr>

              <tr onmouseout="sst_stop()" onmouseover="sst_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='sst_image'>
                      <img src='images/sst/sst.png' width="180">
                    </div>
                    <img src='images/sst/sst.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function sst_start() {
                      document.getElementById('sst_image').style.opacity = "1";
                    }

                    function sst_stop() {
                      document.getElementById('sst_image').style.opacity = "0";
                    }
                    sst_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/tusen-ai/SST">
                    <papertitle>Embracing Single Stride 3D Object Detector with Sparse Transformer
                    </papertitle>
                  </a>
                  <br>
                  Lue Fan, Ziqi Pang,
                  <strong>Tianyuan Zhang</strong>,
                  Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyang Wang, Zhaoxiang Zhang
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://github.com/tusen-ai/SST">github</a> /
                  <a href="https://arxiv.org/abs/2112.06375">arxiv</a> /
                  <p></p>
                  <p>
                    In contrast to 2D, object size in 3D does not exhibit long-tail distributions. We propose a single
                    stride sparse Transformer (SST) for 3D object detection. We obtained impressive results on small
                    objects
                  </p>
                </td>
              </tr>

              <tr onmouseout="detr3d_stop()" onmouseover="det3d_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='detr3d_image'>
                      <img src='images/detr3d/figure_detr3d.png' width="180">
                    </div>
                    <img src='images/detr3d/figure_detr3d.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function detr3d_start() {
                      document.getElementById('pnf_image').style.opacity = "1";
                    }

                    function detr3d_stop() {
                      document.getElementById('pnf_image').style.opacity = "0";
                    }
                    detr3d_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2110.06922">
                    <papertitle>DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries</papertitle>
                  </a> <br>
                  Yue Wang, Vitor Guizilini*,
                  <strong>Tianyuan Zhang*</strong>,
                  Yilun Wang, Hang Zhao, Justin Solomon
                  <br>
                  <em>CoRL</em>, 2021
                  <br>
                  <a href="https://github.com/WangYueFt/detr3d">github</a> /
                  <a href="https://arxiv.org/abs/2110.06922">arxiv</a> /
                  <p>
                    A new paradigm of 3D object detection from multiview 2D images
                  </p>
                </td>
              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='futr3d_image'>
                      <img src='images/futr3d/figure_futr3d.png' width="180">
                    </div>
                    <img src='images/futr3d/figure_futr3d.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('futr3d_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('futr3d_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2203.10642">
                    <papertitle>FUTR3D: A Unified Sensor Fusion Framework for 3D Detection</papertitle>
                  </a>
                  <br>
                  Xuanyao Chen,
                  <strong>Tianyuan Zhang</strong>,
                  Yue Wang, Yilun Wang, Hang Zhao
                  <br>
                  <em>preprint</em>, 2022
                  <br>
                  <a href="https://tsinghua-mars-lab.github.io/futr3d/">project page</a>
                  /
                  <a href="https://github.com/Tsinghua-MARS-Lab/futr3d">github</a> /
                  <a href="https://arxiv.org/abs/2203.10642">arXiv</a>
                  <p></p>
                  <p>
                    A unified framework for 3D detection from multi-sensor data. We achieved impressive results with
                    multiview-cameras and one-beam LiDAR.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='obj365_image'>
                      <img src='images/objects365/figure_objects365.png' width="180">
                    </div>
                    <img src='images/objects365/figure_objects365.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function nerfsuper_start() {
                      document.getElementById('obj365_image').style.opacity = "1";
                    }

                    function nerfsuper_stop() {
                      document.getElementById('obj365_image').style.opacity = "0";
                    }
                    nerfsuper_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.objects365.org/overview.html">
                    <papertitle>Objects365: A Large-scale, High-quality Dataset for Object Detection
                    </papertitle>
                  </a>
                  <br>
                  Shuai Shao*, Zeming Li*,
                  <strong>Tianyuan Zhang*</strong>,
                  Chao Peng*, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun
                  <br>
                  <em>ICCV</em>, 2019
                  <br>
                  <a href="https://www.objects365.org/overview.html">project page</a> /
                  <a
                    href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf">paper</a>
                  <p></p>
                  <p>We provide a <strong>high-quality</strong> large-scale object detection dataset, with 365
                    categories, 638K images,
                    and 10,101K bounding boxes</p>
                </td>
              </tr>

              <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='yopo_image'>
                      <img src='images/yopo/figure_yopo.png' width="180">
                    </div>
                    <img src='images/yopo/figure_yopo.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function refnerf_start() {
                      document.getElementById('yopo_image').style.opacity = "1";
                    }

                    function refnerf_stop() {
                      document.getElementById('yopo_image').style.opacity = "0";
                    }
                    refnerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1905.00877">
                    <papertitle>You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
                    </papertitle>
                  </a>
                  <br>
                  Dinghuai Zhang*,
                  <strong>Tianyuan Zhang*</strong>,
                  Yiping Lu*, Zhanxing Zhu, Bin Dong
                  <br>
                  <em>NeurIPS</em>, 2019 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/1905.00877">arXiv</a>
                  /
                  <a href="https://github.com/a1600012888/YOPO-You-Only-Propagate-Once">code</a>
                  <p></p>
                  <p>Accelerating adversarial training using Pontryagin`s Maximum Principle</p>
                </td>
              </tr>

              <tr onmouseout="atcnn_stop()" onmouseover="atcnn_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='atcnn_image'>
                      <img src='images/atcnn/figure_atcnn.png' width="180">
                    </div>
                    <img src='images/atcnn/figure_atcnn.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function atcnn_start() {
                      document.getElementById('atcnn').style.opacity = "1";
                    }

                    function atcnn_stop() {
                      document.getElementById('atcnn').style.opacity = "0";
                    }
                    atcnn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1905.09797">
                    <papertitle>Interpreting Adversarially Trained Convolutional Neural Networks
                    </papertitle>
                  </a>
                  <br>
                  <strong>Tianyuan Zhang</strong>,
                  Zhanxing Zhu
                  <br>
                  <em>ICML</em>, 2019
                  <br>
                  <a href="https://github.com/PKUAI26/AT-CNN">github</a> /
                  <a href="https://arxiv.org/abs/1905.09797">arXiv</a>
                  <p></p>
                  <p>Discussion on the shape-bias and texture-bias of adversarially trainined convolutional neural
                    networks</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Professional Services</heading>
                  <p>
                    <strong>Program Committee member / Reviewer:</strong> CVPR' 2021, NeurIPS' 2020, ICLR' 2021
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <p font-size:small;>
                    <br>
                    <br>
                  <div style="float:left;">
                    Updated at Nov. 2022
                  </div>
                  <div style="float:right;">
                    <a href="https://jonbarron.info">Template</a>
                  </div>
                  <br>
                  <div style="float:right;">
                    <a href="https://chenceshi.com/">Portfolio Template</a>
                  </div>
                  <br>

                  <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
